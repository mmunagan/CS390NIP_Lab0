{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7nJRfyKWBIJ",
        "outputId": "250f9ac9-6750-4b58-9a18-834e73b5734f"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import random\n",
        "import math\n",
        "from sklearn.metrics import *\n",
        "\n",
        "# Setting random seeds to keep everything deterministic.\n",
        "random.seed(1618)\n",
        "np.random.seed(1618)\n",
        "#tf.set_random_seed(1618)   # Uncomment for TF1.\n",
        "tf.random.set_seed(1618)\n",
        "\n",
        "# Disable some troublesome logging.\n",
        "#tf.logging.set_verbosity(tf.logging.ERROR)   # Uncomment for TF1.\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# Information on dataset.\n",
        "NUM_CLASSES = 10\n",
        "IMAGE_SIZE = 784\n",
        "NEURONS_PER_LAYER = 32\n",
        "HAVE_MINI_BATCHES = False\n",
        "NUM_EPOCHS = 10\n",
        "MBS = 50\n",
        "LR = 0.5\n",
        "\n",
        "# Use these to set the algorithm to use.\n",
        "#ALGORITHM = \"guesser\"\n",
        "ALGORITHM = \"custom_net\"\n",
        "#ALGORITHM = \"tf_net\"\n",
        "\n",
        "class NeuralNetwork_2Layer():\n",
        "    def __init__(self, inputSize, outputSize, neuronsPerLayer, learningRate = 0.1):\n",
        "        self.inputSize = inputSize\n",
        "        self.outputSize = outputSize\n",
        "        self.neuronsPerLayer = neuronsPerLayer\n",
        "        self.lr = learningRate\n",
        "        self.W1 = np.random.randn(self.inputSize, self.neuronsPerLayer)\n",
        "        self.W2 = np.random.randn(self.neuronsPerLayer, self.outputSize)\n",
        "    def mse(self, x, y):\n",
        "        return 0.5 * (x-y)**2\n",
        "\n",
        "    # Activation function.\n",
        "    def __sigmoid(self, x):\n",
        "        return 1/(1+np.exp(-x))\n",
        "\n",
        "    # Activation prime function.\n",
        "    def __sigmoidDerivative(self, x):\n",
        "        return x*(1-x)\n",
        "\n",
        "    # Batch generator for mini-batches. Not randomized.\n",
        "    def __batchGenerator(self, l, n):\n",
        "        for i in range(0, len(l), n):\n",
        "            yield l[i : i + n]\n",
        "\n",
        "    # Training with backpropagation.\n",
        "    def train(self, xVals, yVals, epochs = 10, minibatches = True, mbs = 10):\n",
        "        #TODO: Implement backprop. allow minibatches. mbs should specify the size of each minibatch.\n",
        "        for i in range(epochs):\n",
        "            if not minibatches:\n",
        "                xBatches = xVals\n",
        "                yBatches = yVals\n",
        "            xBatches = self.__batchGenerator(xVals, mbs)\n",
        "            yBatches = self.__batchGenerator(yVals, mbs)\n",
        "            for xBatch, yBatch in zip(xBatches,yBatches):\n",
        "                layer1, layer2 = self.__forward(xBatch)\n",
        "                y_pred = layer2\n",
        "\n",
        "                O_error = (y_pred - yBatch)/len(yBatch)\n",
        "                O_delta = O_error * self.__sigmoidDerivative(y_pred)\n",
        "                self.W2 -= self.lr * layer1.T.dot(O_delta)\n",
        "                \n",
        "                H_error = O_delta.dot(self.W2.T)\n",
        "                H_delta = H_error * self.__sigmoidDerivative(layer1)\n",
        "                self.W1 -= self.lr * xBatch.T.dot(H_delta)\n",
        "                \n",
        "    # Forward pass.\n",
        "    def __forward(self, input):\n",
        "        layer1 = self.__sigmoid(np.dot(input, self.W1))\n",
        "        layer2 = self.__sigmoid(np.dot(layer1, self.W2))\n",
        "        return layer1, layer2\n",
        "\n",
        "    # Predict.\n",
        "    def predict(self, xVals):\n",
        "        _, layer2 = self.__forward(xVals)\n",
        "        return layer2\n",
        "\n",
        "# Classifier that just guesses the class label.\n",
        "def guesserClassifier(xTest):\n",
        "    ans = []\n",
        "    for entry in xTest:\n",
        "        pred = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "        pred[random.randint(0, 9)] = 1\n",
        "        ans.append(pred)\n",
        "    return np.array(ans)\n",
        "\n",
        "\n",
        "\n",
        "#=========================<Pipeline Functions>==================================\n",
        "\n",
        "def getRawData():\n",
        "    mnist = tf.keras.datasets.mnist\n",
        "    (xTrain, yTrain), (xTest, yTest) = mnist.load_data()\n",
        "    print(\"Shape of xTrain dataset: %s.\" % str(xTrain.shape))\n",
        "    print(\"Shape of yTrain dataset: %s.\" % str(yTrain.shape))\n",
        "    print(\"Shape of xTest dataset: %s.\" % str(xTest.shape))\n",
        "    print(\"Shape of yTest dataset: %s.\" % str(yTest.shape))\n",
        "    return ((xTrain, yTrain), (xTest, yTest))\n",
        "\n",
        "\n",
        "\n",
        "def preprocessData(raw):\n",
        "    ((xTrain, yTrain), (xTest, yTest)) = raw            #TODO: Add range reduction here (0-255 ==> 0.0-1.0).\n",
        "    xTrain = xTrain / 255\n",
        "    xTest = xTest / 255\n",
        "    if ALGORITHM == \"custom_net\":\n",
        "        xTrain = xTrain.reshape(len(xTrain), IMAGE_SIZE)\n",
        "        xTest = xTest.reshape(len(xTest), IMAGE_SIZE)\n",
        "    elif ALGORITHM == \"tf_net\":\n",
        "        xTrain = xTrain.reshape(len(xTrain), 28, 28, 1)\n",
        "        xTest = xTest.reshape(len(xTest), 28, 28, 1)\n",
        "    yTrainP = to_categorical(yTrain, NUM_CLASSES)\n",
        "    yTestP = to_categorical(yTest, NUM_CLASSES)\n",
        "    \n",
        "    print(\"New shape of xTrain dataset: %s.\" % str(xTrain.shape))\n",
        "    print(\"New shape of xTest dataset: %s.\" % str(xTest.shape))\n",
        "    print(\"New shape of yTrain dataset: %s.\" % str(yTrainP.shape))\n",
        "    print(\"New shape of yTest dataset: %s.\" % str(yTestP.shape))\n",
        "    return ((xTrain, yTrainP), (xTest, yTestP))\n",
        "\n",
        "def trainModel(data):\n",
        "    xTrain, yTrain = data\n",
        "    if ALGORITHM == \"guesser\":\n",
        "        return None   # Guesser has no model, as it is just guessing.\n",
        "    elif ALGORITHM == \"custom_net\":\n",
        "        print(\"Building and training Custom_NN.\")\n",
        "        nn = NeuralNetwork_2Layer(IMAGE_SIZE, NUM_CLASSES, NEURONS_PER_LAYER, learningRate = LR)\n",
        "        nn.train(xTrain, yTrain, epochs = NUM_EPOCHS, minibatches =  HAVE_MINI_BATCHES, mbs = MBS)\n",
        "        return nn\n",
        "    elif ALGORITHM == \"tf_net\":\n",
        "        print(\"Building and training TF_NN.\")\n",
        "        model = keras.Sequential()\n",
        "        loss = keras.losses.categorical_crossentropy\n",
        "        opt = tf.compat.v1.train.AdamOptimizer()\n",
        "        model.add(keras.layers.Conv2D(NEURONS_PER_LAYER, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
        "        model.add(keras.layers.Conv2D(NEURONS_PER_LAYER, kernel_size=3, activation='relu'))\n",
        "        model.add(keras.layers.Flatten())\n",
        "        model.add(keras.layers.Dense(NUM_CLASSES, activation='softmax'))\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        if HAVE_MINI_BATCHES:\n",
        "            model.fit(xTrain, yTrain, epochs=NUM_EPOCHS, batch_size = MBS)\n",
        "        else:\n",
        "            model.fit(xTrain, yTrain, epochs=NUM_EPOCHS)\n",
        "        return model\n",
        "    else:\n",
        "        raise ValueError(\"Algorithm not recognized.\")\n",
        "\n",
        "def runModel(data, model):\n",
        "    if ALGORITHM == \"guesser\":\n",
        "        return guesserClassifier(data)\n",
        "    elif ALGORITHM == \"custom_net\":\n",
        "        print(\"Testing Custom_NN.\")\n",
        "        return model.predict(data)\n",
        "    elif ALGORITHM == \"tf_net\":\n",
        "        print(\"Testing TF_NN.\")\n",
        "        return model.predict(data)\n",
        "    else:\n",
        "        raise ValueError(\"Algorithm not recognized.\")\n",
        "\n",
        "def evalResults(data, preds):   #TODO: Add F1 score confusion matrix here.\n",
        "    xTest, yTest = data\n",
        "    acc = 0\n",
        "    y_actual = []\n",
        "    y_pred = []\n",
        "    for i in range(preds.shape[0]):\n",
        "        p = np.argmax(preds[i])\n",
        "        y_pred.append(p)\n",
        "        y = np.argmax(yTest[i])\n",
        "        y_actual.append(y)\n",
        "        if p == y:\n",
        "            acc = acc + 1\n",
        "        accuracy = acc / preds.shape[0]\n",
        "    conf_mat = confusion_matrix(y_actual, y_pred, labels=range(0,10))\n",
        "    score = f1_score(y_actual, y_pred, average = \"weighted\")\n",
        "    print(\"Classifier algorithm: %s\" % ALGORITHM)\n",
        "    print(\"Classifier accuracy: %f%%\" % (accuracy * 100))\n",
        "    print(\"Confusion Matrix: \")\n",
        "    print(conf_mat)\n",
        "    print(\"F1 Score: \")\n",
        "    print(score)\n",
        "\n",
        "\n",
        "\n",
        "#=========================<Main>================================================\n",
        "\n",
        "def main():\n",
        "    raw = getRawData()\n",
        "    data = preprocessData(raw)\n",
        "    model = trainModel(data[0])\n",
        "    preds = runModel(data[1][0], model)\n",
        "    evalResults(data[1], preds)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of xTrain dataset: (60000, 28, 28).\n",
            "Shape of yTrain dataset: (60000,).\n",
            "Shape of xTest dataset: (10000, 28, 28).\n",
            "Shape of yTest dataset: (10000,).\n",
            "New shape of xTrain dataset: (60000, 784).\n",
            "New shape of xTest dataset: (10000, 784).\n",
            "New shape of yTrain dataset: (60000, 10).\n",
            "New shape of yTest dataset: (10000, 10).\n",
            "Building and training Custom_NN.\n",
            "Testing Custom_NN.\n",
            "Classifier algorithm: custom_net\n",
            "Classifier accuracy: 89.400000%\n",
            "Confusion Matrix: \n",
            "[[ 944    1    1    2    0   10   12    3    7    0]\n",
            " [   0 1101    4    6    1    1    4    0   17    1]\n",
            " [  19    6  883   22   11    5   16   23   35   12]\n",
            " [   3    1   25  888    1   37    7   15   26    7]\n",
            " [   2    0    6    0  899    4   11    0   11   49]\n",
            " [  18    4    9   46   26  723   18   11   29    8]\n",
            " [  21    4    8    1   15   15  890    1    3    0]\n",
            " [   5   16   27    7   10    3    0  923    7   30]\n",
            " [  14    6   14   29   13   35   23   17  814    9]\n",
            " [   9    7    5   13   46   18    3   24    9  875]]\n",
            "F1 Score: \n",
            "0.8935818560624426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNmqXXsQBdu2"
      },
      "source": [
        ""
      ]
    }
  ]
}